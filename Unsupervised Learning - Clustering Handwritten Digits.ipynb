{
 "metadata": {
  "name": "",
  "signature": "sha256:56455740b8bb28ebf8b1e2973d166c5294cef21fe350787c36209b4b28156e4f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nbprep\n",
      "nbprep.style()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following notebook is a demonstration of [scikit-learn](http://scikit-learn.org/stable/), a Python package for machine learning algorithms. This is not mine (RCN); this is [part](http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/blob/master/Chapter%203%20-%20Unsupervised%20Learning%20-%20Clustering%20Handwritten%20Digits.ipynb) of an on-line [IPython book about machine learning](https://github.com/gmonce/scikit-learn-book). I ran this example but did not proof the text."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Learning Scikit-learn: Machine Learning in Python"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "IPython Notebook for Chapter 3: Unsupervised Learning - Clustering Handwritten Digits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Clustering involves finding groups where all elements in the group are similar, but objects in different groups are not. K-means is the most popular clustering algorithm, because it is very simple and easy to implement and it has shown good performance on different tasks. We will show in this notebook how k-means works using a motivating example, the problem of clustering handwritten digits. At the end of the notebook, we will try other, different, clustering approaches to the same problem _\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using in this chapter. Show the versions we will be using (in case you have problems running the notebooks)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import IPython\n",
      "import sklearn as sk\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print 'IPython version:', IPython.__version__\n",
      "print 'numpy version:', np.__version__\n",
      "print 'scikit-learn version:', sk.__version__\n",
      "print 'matplotlib version:', matplotlib.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import the digits dataset (http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) and show some of its instances"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "from sklearn.preprocessing import scale\n",
      "digits = load_digits()\n",
      "data = scale(digits.data)\n",
      "\n",
      "def print_digits(images,y,max_n=10):\n",
      "    # set up the figure size in inches\n",
      "    fig = plt.figure(figsize=(12, 12))\n",
      "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
      "    i = 0\n",
      "    while i < max_n and i < images.shape[0]:\n",
      "        # plot the images in a matrix of 20x20\n",
      "        p = fig.add_subplot(20, 20, i + 1, xticks=[], yticks=[])\n",
      "        p.imshow(images[i], cmap=plt.cm.bone)\n",
      "        # label the image with the target value\n",
      "        p.text(0, 14, str(y[i]))\n",
      "        i = i + 1\n",
      "    \n",
      "print_digits(digits.images, digits.target, max_n=10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build training and test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training and test set\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "number_of_instances=digits.images.shape[0]\n",
      "# Since from scikit-learn 0.15.1 train_test_split only admits dim=2 arrays\n",
      "# We have to reshape images\n",
      "X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(\n",
      "        data, digits.target, digits.images.reshape(number_of_instances,64),test_size=0.25, random_state=42)\n",
      "\n",
      "n_samples, n_features = X_train.shape\n",
      "n_digits = len(np.unique(y_train))\n",
      "labels = y_train\n",
      "\n",
      "# Reshape images back\n",
      "images_train=images_train.reshape(images_train.shape[0],8,8)\n",
      "images_test=images_test.reshape(images_test.shape[0],8,8)\n",
      "\n",
      "\n",
      "print_digits(images_train, y_train, max_n=20)\n",
      "print_digits(images_test, y_test, max_n=20)\n",
      "\n",
      "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
      "      % (n_digits, n_samples, n_features))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "K-Means"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The main idea behind k-means is to find a partition of data points such that the squared distance between the cluster mean and each point in the cluster is minimized. Note that this method assumes that you know a priori the number of clusters your data should be divided into. Train a KMeans classifier, show the clusters. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cluster\n",
      "clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)\n",
      "clf.fit(X_train)\n",
      "print clf.labels_.shape\n",
      "print clf.labels_[1:10]\n",
      "print_digits(images_train, clf.labels_, max_n=10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To predict the clusters for training data, we use the usual predict method of the classifier. Predict and show predicted clusters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict clusters on testing data\n",
      "y_pred = clf.predict(X_test)\n",
      "\n",
      "def print_cluster(images, y_pred, cluster_number):\n",
      "    images = images[y_pred==cluster_number]\n",
      "    y_pred = y_pred[y_pred==cluster_number]\n",
      "    print_digits(images, y_pred, max_n=10)\n",
      "\n",
      "for i in range(10):\n",
      "     print_cluster(images_test, y_pred, i)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How can we evaluate our performance? Precision and all that stuff does not work, since we have no target classes to compare with. To evaluate, we need to know the \"real\" clusters, whatever that means. We can suppose, for our example, that each cluster includes every drawing of a certain number, and only that number. Knowing this, we can compute the adjusted Rand index between our cluster assignment and the expected one. The Rand index is a similar measure for accuracy, but it takes into account the fact that classes can have different names in both assignments. That is, if we change class names, the index does not change. The adjusted index tries to deduct from the result coincidences that have occurred by chance. When you have the exact same clusters in both sets, the Rand index equals one, while it equals zero when there are no clusters sharing a data point. Show different performance metrics, compared with \"original\" clusters (using the knowb number class)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "print \"Addjusted rand score:{:.2}\".format(metrics.adjusted_rand_score(y_test, y_pred))\n",
      "print \"Homogeneity score:{:.2} \".format(metrics.homogeneity_score(y_test, y_pred)) \n",
      "print \"Completeness score: {:.2} \".format(metrics.completeness_score(y_test, y_pred))\n",
      "print \"Confusion matrix\"\n",
      "print metrics.confusion_matrix(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Draw clusters and centroids (taken from [the scikit-learn tutorial](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import decomposition\n",
      "# in this case the seeding of the centers is deterministic, hence we run the\n",
      "# kmeans algorithm only once with n_init=1\n",
      "pca = decomposition.PCA(n_components=2).fit(X_train)\n",
      "reduced_X_train = pca.transform(X_train)\n",
      "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
      "h = .01     # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "\n",
      "# Plot the decision boundary. For that, we will asign a color to each\n",
      "x_min, x_max = reduced_X_train[:, 0].min() + 1, reduced_X_train[:, 0].max() - 1\n",
      "y_min, y_max = reduced_X_train[:, 1].min() + 1, reduced_X_train[:, 1].max() - 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "\n",
      "kmeans = cluster.KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
      "kmeans.fit(reduced_X_train)\n",
      "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)\n",
      "plt.figure(1)\n",
      "plt.clf()\n",
      "plt.imshow(Z, interpolation='nearest',\n",
      "          extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
      "          cmap=plt.cm.Paired,\n",
      "          aspect='auto', origin='lower')\n",
      "#print reduced_X_train.shape\n",
      "\n",
      "plt.plot(reduced_X_train[:, 0], reduced_X_train[:, 1], 'k.', markersize=2)\n",
      "# Plot the centroids as a white X\n",
      "centroids = kmeans.cluster_centers_\n",
      "\n",
      "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
      "           marker='.', s=169, linewidths=3,\n",
      "           color='w', zorder=10)\n",
      "\n",
      "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
      "         'Centroids are marked with white dots')\n",
      "plt.xlim(x_min, x_max)\n",
      "plt.ylim(y_min, y_max)\n",
      "plt.xticks(())\n",
      "plt.yticks(())\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Affinity Propagation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A typical problem for clustering is that most methods require the number of clusters we want to identify. The general approach to solve this is to try different numbers and let an expert determine which works best using techniques such as dimensionality reduction to visualize clusters. There are also some methods that try to automatically calculate the number of clusters. Scikit-learn includes an implementation of Affinity Propagation, a method that looks for instances that are the most representative of others, and uses them to describe the clusters. Since scikit-learn uses the same function for every algorithm, we just have to fit the training set again:Try now using Affinity Propagation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Affinity propagation\n",
      "aff = cluster.AffinityPropagation()\n",
      "aff.fit(X_train)\n",
      "print aff.cluster_centers_indices_.shape\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_digits(images_train[aff.cluster_centers_indices_], y_train[aff.cluster_centers_indices_], max_n=aff.cluster_centers_indices_.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "MeanShift"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yet another clustering method: MeanShift()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#MeanShift\n",
      "ms = cluster.MeanShift()\n",
      "ms.fit(X_train)\n",
      "print ms.cluster_centers_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ms.cluster_centers_.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mixture of Gaussian Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we will try a probabilistic approach to clustering, using Gaussian Mixture Models (GMM). We will see, from a procedural view, that it is very similar to k-means, but their theoretical principles are quite different. GMM assumes that data comes from a mixture of finite Gaussian distributions with unknown parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will first use a heldout dataset to estimate covariance type (one of the method parameters)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import mixture\n",
      "\n",
      "# Define a heldout dataset to estimate covariance type\n",
      "X_train_heldout, X_test_heldout, y_train_heldout, y_test_heldout = train_test_split(\n",
      "        X_train, y_train,test_size=0.25, random_state=42)\n",
      "for covariance_type in ['spherical','tied','diag','full']:\n",
      "    gm=mixture.GMM(n_components=n_digits, covariance_type=covariance_type, random_state=42, n_init=5)\n",
      "    gm.fit(X_train_heldout)\n",
      "    y_pred=gm.predict(X_test_heldout)\n",
      "    print \"Adjusted rand score for covariance={}:{:.2}\".format(covariance_type, metrics.adjusted_rand_score(y_test_heldout, y_pred))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gm = mixture.GMM(n_components=n_digits, covariance_type='tied', random_state=42)\n",
      "gm.fit(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print train clustering and confusion matrix\n",
      "y_pred = gm.predict(X_test)\n",
      "print \"Addjusted rand score:{:.2}\".format(metrics.adjusted_rand_score(y_test, y_pred))\n",
      "print \"Homogeneity score:{:.2} \".format(metrics.homogeneity_score(y_test, y_pred)) \n",
      "print \"Completeness score: {:.2} \".format(metrics.completeness_score(y_test, y_pred))\n",
      "for i in range(10):\n",
      "     print_cluster(images_test, y_pred, i)\n",
      "print \"Confusion matrix\"\n",
      "print metrics.confusion_matrix(y_test, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Predict!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl=plt\n",
      "from sklearn import decomposition\n",
      "# in this case the seeding of the centers is deterministic, hence we run the\n",
      "# kmeans algorithm only once with n_init=1\n",
      "pca = decomposition.PCA(n_components=2).fit(X_train)\n",
      "reduced_X_train = pca.transform(X_train)\n",
      "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
      "h = .01     # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "\n",
      "# Plot the decision boundary. For that, we will asign a color to each\n",
      "# Taken from \n",
      "x_min, x_max = reduced_X_train[:, 0].min() + 1, reduced_X_train[:, 0].max() - 1\n",
      "y_min, y_max = reduced_X_train[:, 1].min() + 1, reduced_X_train[:, 1].max() - 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "\n",
      "gm.fit(reduced_X_train)\n",
      "#print np.c_[xx.ravel(),yy.ravel()]\n",
      "Z = gm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)\n",
      "pl.figure(1)\n",
      "pl.clf()\n",
      "pl.imshow(Z, interpolation='nearest',\n",
      "          extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
      "          cmap=pl.cm.Paired,\n",
      "          aspect='auto', origin='lower')\n",
      "#print reduced_X_train.shape\n",
      "\n",
      "pl.plot(reduced_X_train[:, 0], reduced_X_train[:, 1], 'k.', markersize=2)\n",
      "# Plot the centroids as a white X\n",
      "centroids = gm.means_\n",
      "\n",
      "pl.scatter(centroids[:, 0], centroids[:, 1],\n",
      "           marker='.', s=169, linewidths=3,\n",
      "           color='w', zorder=10)\n",
      "\n",
      "pl.title('Mixture of gaussian models on the digits dataset (PCA-reduced data)\\n'\n",
      "         'Means are marked with white dots')\n",
      "pl.xlim(x_min, x_max)\n",
      "pl.ylim(y_min, y_max)\n",
      "pl.xticks(())\n",
      "pl.yticks(())\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}