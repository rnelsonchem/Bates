{
 "metadata": {
  "name": "",
  "signature": "sha256:36b13b14f6cbb850de6331c8a609fe8b177b6c7e493ff8e16fe5b1e65d3fe06b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import nbprep\n",
      "nbprep.style()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Advanced Data Tables\n",
      "\n",
      "This is a more advanced example of data analysis with Pandas. The data sets are microbial growth information as measured by [UV/Vis spectroscopy](http://en.wikipedia.org/wiki/Ultraviolet%E2%80%93visible_spectroscopy) from a series of experiments in a 96-well plate. This is not my data or analysis. I found this example in a [blog post](http://bconnelly.net/2014/04/analyzing-microbial-growth-with-r/) by Dr. Brian Connelly. His example is in R, and is included in a code block at the very end of this notebook. I've rewritten this analysis in Pandas, and I'm sharing it here with his permission. This example highlights the similarities and differences between Pandas and R. Please read his blog post for a detailed description of the problem, data collection, and analysis.\n",
      "\n",
      "## The Python verison\n",
      "\n",
      "Import some third party libraries. Set Pandas plotting to look like R's ggplot2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option('display.mpl_style', 'default')\n",
      "import scipy.stats as sps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we are reading in the raw data from the 96-well plate. It is a huge file (>430,000 pieces of data), as you can see from the shape of the imported DataFrame. The column headings 'A1', 'A2', etc. refer to the different wells in the plate; the values are the UV/Vis absorbance at that position/time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rawdata = pd.read_csv('static/raw.csv')\n",
      "print rawdata.shape\n",
      "rawdata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to reformat this data block for the next step of the analysis. In this case, we are taking all of the data under the well columns and putting that into a single column. To do this, we use [Pandas' `melt` function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html). `id_vars` are identifying data for each value; `var_name` will be the column heading for our column headings; and `OD600` are the table entries at that location. Examining the new table makes it clear what has happened."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reshaped = pd.melt(rawdata, id_vars=[\"Time\", \"Temperature\"], var_name=\"Well\", \n",
      "                   value_name=\"OD600\")\n",
      "\n",
      "reshaped"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each well in the 96-well plate corresponds to a different set of experimental conditions, such as microbial strain and environment. A table of these conditions is saved in a separate file called \"platemap.csv\". Some of these are controls (NaN values)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "platemap = pd.read_csv('static/platemap.csv')\n",
      "\n",
      "platemap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'd like to be able to map these values onto our large data set, thereby labeling all of the data with experimental information. We can do that easily with [Pandas' `merge` function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html), which is one of Pandas' many advanced [SQL-like merging, joining, and concatenating capabilities](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html). In this case, we want to match the two tables based on the values in the \"Well\" column, which has the same name in both tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotated = pd.merge(reshaped, platemap, left_on='Well', right_on='Well',\n",
      "        how='inner', sort=False)\n",
      "\n",
      "annotated"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we'll regroup all of this data on the basis of several different variables. We can then aggregate ([`agg` function](http://pandas.pydata.org/pandas-docs/stable/groupby.html)) the grouped data in several different ways: number of data points (`np.size`), the average (`np.mean`), and the standard deviation (`np.std`). Remember the result is a [hierarchical indexed table](http://pandas.pydata.org/pandas-docs/stable/indexing.html#hierarchical-indexing-multiindex)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grouped = annotated.groupby([\"Environment\", \"Strain\", \"Time\"])\n",
      "stats = grouped['OD600'].agg({\"N\":np.size,\"Average\":np.mean,\"Std\":np.std})\n",
      "\n",
      "stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we'd also like to add a column with the [confidence interval](http://en.wikipedia.org/wiki/Confidence_interval). To calculate this value, we need to know the [Student's t](http://en.wikipedia.org/wiki/Student%27s_t-distribution) value for a particular confidence level (95% in this case) and number of degrees of freedom. We can use [Scipy's stats module](http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) to determine this value. The [`t` object](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html#scipy.stats.t) has a percent point function (`ppf`) that gives t values at a given confidence level (in this case 0.025 = (1.-0.95)/2) for a variety of degrees of freedom (`df`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sps.t.ppf(0.025, df=[9, 8, 7])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now calculate a new 'Error' column using some of the other columns from our table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stats['Error'] = -1*sps.t.ppf(0.025, df=stats['N']-1)*stats['Std']/np.sqrt(stats['N'])\n",
      "\n",
      "stats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# These are some other methods to do the same thing; however, these are all much slower.\n",
      "# Also: -1*sps.t.ppf(0.025, df=n-1)\n",
      "# Also: create sps.t(df=0) class instance and change t.kwds['df']=n-1 and call t.ppf(0.025)\n",
      "# def CI95(data):\n",
      "#     n = len(data)\n",
      "#     error = sps.t.interval(0.95, df=n-1)[1]*np.std(data)/np.sqrt(n)\n",
      "#     return error\n",
      "# stats = grouped['OD600'].agg({\"N\":len,\"Average\":np.mean,\"Std\":np.std, \"Error\":CI95})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've done all of this analysis, it is useful to unstack the grouped data for plotting. In this case, we'll convert the 'Strain' index (`level=1`) into grouped columns. \n",
      "\n",
      "Our index (rows) now only has two levels rather than three."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ustack = stats.unstack(level=1)\n",
      "\n",
      "print ustack.index.levels\n",
      "ustack"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can make a plot of one of the Environments (`i`) pretty easily. (See the first set of indices from the output above to find the available values for `i`.) Comments are include in the plotting code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make this a rather large plot.\n",
      "fig = plt.figure(figsize=(9,9))\n",
      "\n",
      "i = 1\n",
      "# Here we are selecting the Averages and Errors from our table above. \n",
      "# This is mostly just a convenience so that we don't have to rewrite this all the time\n",
      "data = ustack.loc[i,\"Average\"]\n",
      "errors = ustack.loc[i, \"Error\"]\n",
      "# The index of \"data\" are the times in seconds. Convert them to hours. \n",
      "times = data.index/3600.\n",
      "# Determine the upper and lower bounds or our confidence interval around the mean\n",
      "upper = data + errors\n",
      "lower = data - errors\n",
      "\n",
      "# The columns of our data are now the strain information\n",
      "for strain in data.columns:\n",
      "    # Make a fill_between plot for the errors. It will be transparent black, \n",
      "    # which will look gray\n",
      "    plt.fill_between(times, lower[strain], upper[strain], color='k', alpha=0.5, lw=0)\n",
      "    # Plot the data vs times. Give it a strain label for the legend.\n",
      "    plt.plot(times, data[strain], label='Strain {}'.format(strain))\n",
      "# Title the plot and add a legend\n",
      "plt.title('Environment {:d}'.format(i))\n",
      "plt.legend(loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also make a stack plot of all the environments using a simple loop and the above plotting code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(9,9))\n",
      "for i in ustack.index.levels[0]:\n",
      "    # Subplot takes care of arranging the required number of plotting axes\n",
      "    # In this case we want 3 rows and 1 column\n",
      "    plt.subplot(3, 1, i)\n",
      "    data = ustack.loc[i,\"Average\"]\n",
      "    errors = ustack.loc[i, \"Error\"]\n",
      "    times = data.index/3600.\n",
      "    upper = data + errors\n",
      "    lower = data - errors\n",
      "    for strain in data.columns:\n",
      "        plt.fill_between(times, lower[strain], upper[strain], color='k', alpha=0.5, lw=0)\n",
      "        plt.plot(times, data[strain], label='Strain {}'.format(strain))\n",
      "    plt.title('Environment {:d}'.format(i))\n",
      "\n",
      "# We only want a single legend for all of the plots\n",
      "plt.figlegend(plt.gca().get_lines(), data.columns, 'right')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bringing it all together\n",
      "\n",
      "Below is the complete code for this example. Uncomment the first and last line if you want to time this example. On my computer, I get ~1 second for all of this to run. (Imports can be slow, but they are only done once in Python. Importing a module a second time in your code will do nothing, so it appears much faster.) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%timeit\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option('display.mpl_style', 'default')\n",
      "import scipy.stats as sps\n",
      "\n",
      "rawdata = pd.read_csv('static/raw.csv')\n",
      "platemap = pd.read_csv('static/platemap.csv')\n",
      "\n",
      "reshaped = pd.melt(rawdata, id_vars=[\"Time\", \"Temperature\"], var_name=\"Well\", \n",
      "                   value_name=\"OD600\")\n",
      "annotated = pd.merge(reshaped, platemap, left_on='Well', right_on='Well',\n",
      "        how='inner', sort=False)\n",
      "\n",
      "grouped = annotated.groupby([\"Environment\", \"Strain\", \"Time\"])\n",
      "stats = grouped['OD600'].agg({\"N\":len, \"Average\":np.mean,\"Std\":np.std})\n",
      "stats['Error'] = -1*sps.t.ppf(0.025, df=stats['N']-1)\\\n",
      "        *stats['Std']/np.sqrt(stats['N'])\n",
      "\n",
      "ustack = stats.unstack(level=1)\n",
      "fig = plt.figure(figsize=(9,9))\n",
      "for i in (ustack.index.levels[0]):\n",
      "    plt.subplot(3, 1, i)\n",
      "    data = ustack.loc[i,\"Average\"]\n",
      "    errors = ustack.loc[i, \"Error\"]\n",
      "    times = data.index/3600.\n",
      "    upper = data + errors\n",
      "    lower = data - errors\n",
      "    for strain in data.columns:\n",
      "        plt.fill_between(times, lower[strain], upper[strain], color='k', alpha=0.5, lw=0)\n",
      "        plt.plot(times, data[strain], label='Strain {}'.format(strain))\n",
      "    plt.title('Environment {:d}'.format(i))\n",
      "\n",
      "plt.figlegend(plt.gca().get_lines(), data.columns, 'right')\n",
      "#plt.close() # Don't show all the plots for timing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Run the orignial example\n",
      "\n",
      "We can run the original example directly in R with the rpy2 module. In this case, the local R instance must have the modules \"reshape2\", \"dplyr\", and \"ggplot2\" installed. There are instructions to [do this in SMC](https://github.com/sagemath/cloud/wiki/FAQ#-question-i-would-like-to-install-new-r-packages).\n",
      "\n",
      "Note. You can time the R example by first saving the following code to a new file (*filename*). From the R from the terminal, type system.time( source(*filename*)) for the timing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rpy2.ipython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(reshape2)\n",
      "library(dplyr)\n",
      "library(ggplot2)\n",
      "\n",
      "# Read in the raw data and the platemap. You may need to first change your\n",
      "# working directory with the setwd command.\n",
      "rawdata <- read.csv(\"static/raw.csv\")\n",
      "platemap <- read.csv(\"static/platemap.csv\")\n",
      "\n",
      "# Reshape the data. Instead of rows containing the Time, Temperature,\n",
      "# and readings for each Well, rows will contain the Time, Temperature, a\n",
      "# Well ID, and the reading at that Well.\n",
      "reshaped <- melt(rawdata, id=c(\"Time\", \"Temperature\"), variable.name=\"Well\", \n",
      "                 value.name=\"OD600\")\n",
      "\n",
      "# Add information about the experiment from the plate map. For each Well\n",
      "# defined in both the reshaped data and the platemap, each resulting row\n",
      "# will contain the absorbance measurement as well as the additional columns\n",
      "# and values from the platemap.\n",
      "annotated <- inner_join(reshaped, platemap, by=\"Well\")\n",
      "\n",
      "# Save the annotated data as a CSV for storing, sharing, etc.\n",
      "# write.csv(annotated, \"data-annotated.csv\")\n",
      "\n",
      "conf_int95 <- function(data) {\n",
      "    n <- length(data)\n",
      "    error <- qt(0.975, df=n-1) * sd(data)/sqrt(n)\n",
      "    return(error)\n",
      "}\n",
      "\n",
      "# Group the data by the different experimental variables and calculate the\n",
      "# sample size, average OD600, and 95% confidence limits around the mean\n",
      "# among the replicates. Also remove all records where the Strain is NA.\n",
      "stats <- annotated %.%\n",
      "              group_by(Environment, Strain, Time) %.%\n",
      "              summarise(N=length(OD600),\n",
      "                        Average=mean(OD600),\n",
      "                        CI95=conf_int95(OD600)) %.%\n",
      "              filter(!is.na(Strain))\n",
      "\n",
      "# Plot the average OD600 over time for each strain in each environment\n",
      "ggplot(data=stats, aes(x=Time/3600, y=Average, color=Strain)) +\n",
      "       geom_ribbon(aes(ymin=Average-CI95, ymax=Average+CI95, fill=Strain),\n",
      "                   color=NA, alpha=0.3) + \n",
      "       geom_line() +\n",
      "       scale_y_log10() +\n",
      "       facet_grid(Environment ~ .) +\n",
      "       labs(x=\"Time (Hours)\", y=\"Absorbance at 600 nm\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}